---
title: 'How I improve training epoch time from 4 hrs to 0.5 hr'
date: 2022-05-30
permalink: /posts/2022/05/How_I_improve_training/
tags:
  - GPU
  - Deeplearning
  - Data preprocessing
---


## 问题描述
直接使用mmhuman3d的代码，用agora dataset训练hmr, 发现GPU的利用率极低，大部分时间都是0%.而且训练一个epoch大概要5小时，远慢于官方的半小时。
使用torch profiler发现训练的操作并无问题, 一个batch 零点几s。在dataloader里使用print,发现__getitem__里一个数据预处理大概要2s。
由此定位到，是dataloader预处理数据的时间导致了训练效率低。
这里大概说一下dataloader流程。
创建dataloder的时候，首先init阶段CPU会把数据都读到RAM中（跟GPU无关）。在runtime iterate dataloader的时候，如果使用torch distributed training(实现参考multiprocessing), dataloader会复制出num_workers个子进程（linux的默认创建子进程的方法是fork, windows/macos是spawn, fork就相当于完整复制父进程的资源，spawn只复制必要的资源，因此spawn start慢，但消耗资源少, 很多代码会手动set start method 为spawn)，这些子进程的任务就是完成__getitem__的过程，也就是取原始数据，预处理，组合成batch，返回给主进程。注意2点：
  1. num_workers=0但依然使用distribute/multiprocessing的时候，主进程会完成全部的工作，但是这样会导致主进程cpu使用率非常高(2000%)。但如果workers>=1, 每个进程的cpu使用率都不会超过100%, 具体原因未知。
  2. 每fork出一个子进程的时候，在init时被创建的dataset部分是会在RAM里被完整复制一遍的（线程资源相互隔离），这可能会导致一些问题，接下来会提。
 
定位了第一个问题后，自然方法就是先把数据预处理好存下来，getitem的时候直接load。因此我提前完成了图片的切片，数据的转换等工作，均使用npz格式存下来，load回去就可以直接用于训练。
但是这里留下了一个隐患。数据包括输入（图片）和标注（labels)两部分，图片都是getitem的时候一张张读取的，但标注我根据commonsense, 在init阶段就读进了dataset, getitem的时候直接index。我认为，标注占的存储空间不大，这样做能最大程度节省getitem的时间，而且我见过的所有代码都是这样做的。然而，我忽略一个致命的疏漏。别的代码的标注都是一个dataset存在一个文件里的，而我是每一个数据存了一个npz file。即使是同样的数据，别人的npz file总共～250M,但我用for循环读进来的npz file总共～13G。本来，多占一些内存也不是什么大问题，毕竟现在的机器最少也有上百G RAM, 13G RAM即使使用8个worker理论上也没有问题。但是，这里又出现了大文件幽灵问题：当num_workers > 0的时候，会报mmap cannot allocate memory的错误，然而机器上的RAM空余的空间远远大于所需的空间。这个大文件问题在polipo也遇到过类似的，使用polipo下载，超过2G的时候polipo会自动崩溃，polipo的维基百科上也有记载，但没有说原因。我调查排除了很多原因，但依然没有找到确定的原因，一个可能的原因是linux的popen函数（涉及到pipe，fork，mmap)存在某些系统上的限制(目前已知除了RAM之外，file discriptor(getconf OPEN_MAX)的数量，pipe的数量（getconf STREAM_MAX)都可能导致mmap失败）。但总而言之，问题就是在于init以后，dataset过大了（目前的经验就是不要超过10G), 导致创建子进程的时候失败。

因此，接下来的解决方案就是，init的时候只读进标注的path, getitem的时候再读取。这样，num_workers > 0不再出现问题了，GPU利用率也上去了，CPU利用率也不会过高了。
## 结论总结
## 过程日志
